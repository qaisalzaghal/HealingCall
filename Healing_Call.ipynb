{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qaisalzaghal/HealingCall/blob/main/Healing_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SD2E3Lyny-8N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ur1_MozmtO-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Collecting data**"
      ],
      "metadata": {
        "id": "6wRYgB-jy_wW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfcIW1lpthDj"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/Symptom2Disease.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opvd2Yzqybg5"
      },
      "outputs": [],
      "source": [
        "df.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gsal67Butlde"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_4_djaxtp-w"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYqmMP0Wt0Di"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKftpIF3uATb"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbrJBauHuEpE"
      },
      "outputs": [],
      "source": [
        "df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore data balance"
      ],
      "metadata": {
        "id": "Rot9Vw4e0qBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0vnoZ5LuxFV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "ax = sns.barplot(x=df[\"label\"].value_counts().index,\n",
        "                 y=df[\"label\"].value_counts(),\n",
        "                 palette=\"rocket\")\n",
        "plt.xlabel(\"Disease\", fontsize=12)\n",
        "plt.ylabel(\"Count\", fontsize=12)\n",
        "plt.title(\"Count of Diseases\", size=16)\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "counts = df[\"label\"].value_counts()\n",
        "plt.pie(counts,\n",
        "        labels=counts.index,\n",
        "        startangle=90,\n",
        "        wedgeprops={'edgecolor': 'white', 'linewidth': 1})\n",
        "plt.title(\"Distribution of Diseases\", size=16)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take \"text\" as feature, \"label\" as label and drop \"Unnamed: 0\""
      ],
      "metadata": {
        "id": "DL7vs_cW0z9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iibYc8hUxjOR"
      },
      "outputs": [],
      "source": [
        "df=df.drop(\"Unnamed: 0\",axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "776k__96xm6f"
      },
      "outputs": [],
      "source": [
        "#df.to_csv('new_dataframe_disease.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpZUqSljx-vt"
      },
      "outputs": [],
      "source": [
        "#df=pd.read_csv(\"new_dataframe_disease.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "AzTKwn-n1e77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay6R7RUKz6pI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove stopwords and punctuations"
      ],
      "metadata": {
        "id": "Jy6PTKFv1lRA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUbXnrPQz_51"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_data(text):\n",
        "  text =''.join([char for char in text if char not in string.punctuation])\n",
        "  text=' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "  text=text.lower()\n",
        "  words = word_tokenize(text)\n",
        "  return \" \".join(words)\n",
        "\n",
        "df[\"cleaned_text\"] = df[\"text\"].apply(preprocess_data)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBQ0e27d1E_O"
      },
      "outputs": [],
      "source": [
        "#df.to_csv('new_dataframe_myproject_preprocess.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N3yuSidi0eT"
      },
      "outputs": [],
      "source": [
        "#df=pd.read_csv(\"new_dataframe_myproject_preprocess.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## text length before and after preprocessing"
      ],
      "metadata": {
        "id": "mcUJk_p71uT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyNLpif7i_kL"
      },
      "outputs": [],
      "source": [
        "df[\"text_length\"] = df[\"text\"].apply(lambda x:len(x.split()))\n",
        "df[\"cleaned_text_length\"] = df[\"cleaned_text\"].apply(lambda x:len(x.split()))\n",
        "df=df[[\"text\",\"text_length\",\"cleaned_text\",\"cleaned_text_length\",\"label\"]]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oW4TWgEOFjP"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig=plt.figure(figsize=(10, 6))\n",
        "fig.add_subplot(1, 2, 1)\n",
        "sns.set_style(\"whitegrid\")\n",
        "x_labels = [\"text_length\", \"cleaned_text_length\"]\n",
        "y_values = [df[\"text_length\"].sum(), df[\"cleaned_text_length\"].sum()]\n",
        "sns.barplot(x=x_labels, y=y_values)\n",
        "plt.title(\"Total Word Count\")\n",
        "for i, value in enumerate(y_values):\n",
        "    plt.text(i, value , str(value), ha='center', va='bottom')\n",
        "\n",
        "\n",
        "fig.add_subplot(1, 2, 2)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.hist(df[\"text_length\"], bins=50, label=\"Original Text\")\n",
        "plt.hist(df[\"cleaned_text_length\"], bins=50, alpha=0.5, label=\"Cleaned Text\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Count Distribution\")\n",
        "plt.legend()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---WordCloud--- before and after preprocessing"
      ],
      "metadata": {
        "id": "BPSzCNan12vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4CBQkpfkOQB"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "text = \" \".join(df[\"text\"].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"black\", stopwords=STOPWORDS).generate(text)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Text\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "text_cleaned = \" \".join(df[\"cleaned_text\"].astype(str).tolist())\n",
        "wordcloud_cleaned = WordCloud(width=800, height=400, background_color=\"black\", stopwords=STOPWORDS).generate(text_cleaned)\n",
        "plt.imshow(wordcloud_cleaned)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Cleaned Text\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Lfe0aKkb_8"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "\n",
        "def get_lemmatized(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "  return \" \".join(lemmatized_words)\n",
        "\n",
        "df[\"lemmatized_text\"] = df[\"cleaned_text\"].apply(get_lemmatized)\n",
        "\n",
        "def get_stemmed(text):\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
        "  return \" \".join(stemmed_words)\n",
        "\n",
        "df[\"stemmed_text\"] = df[\"lemmatized_text\"].apply(get_stemmed)\n",
        "\n",
        "df=df[[\"text\",\"text_length\",\"cleaned_text\",\"cleaned_text_length\",\"lemmatized_text\",\"stemmed_text\",\"label\"]]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56xnOqLNVj-y"
      },
      "outputs": [],
      "source": [
        "#!pip install gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preparing data for ML models"
      ],
      "metadata": {
        "id": "wjAfJ3VD2NP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4A-GCG7k1vW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GU1WV6MimCKH"
      },
      "outputs": [],
      "source": [
        "#df.to_csv('new_dataframe_myproject_stemmed_and_labeled.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epO5PV8QmpJp"
      },
      "outputs": [],
      "source": [
        "#df=pd.read_csv(\"new_dataframe_myproject_stemmed_and_labeled.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIqAFeTibCpl"
      },
      "outputs": [],
      "source": [
        "#!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Sentence Transformer Vectorization"
      ],
      "metadata": {
        "id": "WjtYfLa12SZq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trEt4wmumtYg"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "texts = df['cleaned_text'] #text\n",
        "labels = df['label']\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7c7K9Ve0aWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPBvRG1EqwHJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "np.save(\"myproject_embeddings_using(all-MiniLM-L6-v2).npy\", embeddings)\n",
        "df['label'].to_csv(\"myproject_labels.csv\", index=False)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "X = np.load(\"myproject_embeddings_using(all-MiniLM-L6-v2).npy\")\n",
        "y = pd.read_csv(\"myproject_labels.csv\").values.ravel()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Logistic Regression"
      ],
      "metadata": {
        "id": "38DudvGh2hfM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfopmWLnqpcd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "scores_LogisticRegression = cross_val_score(clf,  X_train, y_train, cv=5)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_LogisticRegression = clf.predict(X_test)\n",
        "\n",
        "print(\"Cross-validation scores:\", scores_LogisticRegression)\n",
        "print(\"Average Accuracy:\", scores_LogisticRegression.mean())\n",
        "print(\"Best Accuracy:\", scores_LogisticRegression.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training Random Forest model"
      ],
      "metadata": {
        "id": "ROdJ1yId2k5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeM_xecut9zi"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "scores_RandomForestClassifier = cross_val_score(clf_1,  X_train, y_train, cv=5)\n",
        "\n",
        "print(\"Cross-validation scores:\", scores_RandomForestClassifier)\n",
        "print(\"Average Accuracy:\", np.mean(scores_RandomForestClassifier))\n",
        "print(\"Best Accuracy:\", scores_RandomForestClassifier.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using Gris Search CV to inhuncing the Logistic regression model"
      ],
      "metadata": {
        "id": "lCB470zi2ubK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzFhjp1tuj5m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "grid.fit( X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best accuracy:\", grid.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare the accuracy of all models"
      ],
      "metadata": {
        "id": "X_sH81UQ3Idh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9Wg7fKFkvR0"
      },
      "outputs": [],
      "source": [
        "score_values=[np.mean(scores_LogisticRegression), np.mean(scores_RandomForestClassifier), grid.best_score_]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.bar(\n",
        "    [\"LogisticRegression\", \"Random Forest\", \"GridSearchCV\"],\n",
        "    score_values,\n",
        "    color = ['darkred', 'midnightblue', 'darkgreen']\n",
        ")\n",
        "for i, value in enumerate(score_values):\n",
        "    plt.text(i, value + 0.01 , f\"{value * 100:.0f}%\" , ha='center', va='bottom',size=14)\n",
        "\n",
        "plt.xlabel(\"Models\",size=13)\n",
        "plt.ylabel(\"Accuracy\",size=13)\n",
        "plt.title(\"Model Accuracy Comparison\",size=15)\n",
        "plt.ylim(0, 1)\n",
        "plt.yticks(np.arange(0.1, 1.0, 0.05))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evyhs6ILu1N6"
      },
      "outputs": [],
      "source": [
        "#import joblib\n",
        "\"\"\"\n",
        "joblib.dump(grid, 'myproject_grid_model_trained.joblib')\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "loaded_model = joblib.load('myproject_grid_model_trained.joblib')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gResN6DCITom"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "joblib.dump(X_train, 'X_train_myproject.pkl')\n",
        "joblib.dump(X_test, 'X_test_myproject.pkl')\n",
        "joblib.dump(y_train, 'y_train_myproject.pkl')\n",
        "joblib.dump(y_test, 'y_test_myproject.pkl')\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "X_train = joblib.load('X_train_myproject.pkl')\n",
        "X_test = joblib.load('X_test_myproject.pkl')\n",
        "y_train = joblib.load('y_train_myproject.pkl')\n",
        "y_test = joblib.load('y_test_myproject.pkl')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eG4mTx5vY5P"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAqWTtfl2f26"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\", size=20)\n",
        "plt.xlabel(\"Predicted\", size=14)\n",
        "plt.ylabel(\"Actual\", size=14)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkpNToqEEYeP"
      },
      "outputs": [],
      "source": [
        "df_1=pd.read_csv(\"/content/Symptom2Disease.csv\")\n",
        "label_count_dict = dict(zip(df[\"label\"].value_counts().index, df_1[\"label\"].value_counts().index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_7g9m6k28tf"
      },
      "outputs": [],
      "source": [
        "test_texts = [\"I feel very dizzy and tired, my head hurts me very much\"]\n",
        "test_texts_1=[\"I feel pain, swelling and heaviness in the legs, and it is often accompanied by a burning sensation and discomfort.\"]\n",
        "test_texts_2=[\"I sneeze a lot, my eyes itch and water, and my skin breaks out in rashes after exposure to allergens.\"]\n",
        "test_texts_3=[\"I feel itchy red spots all over my body, with fever, fatigue, and blisters that burst and form scabs.\"]\n",
        "\n",
        "def pred(text):\n",
        "  model_test = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "  test_embeddings = model_test.encode(text)\n",
        "\n",
        "  output = grid.predict(test_embeddings)\n",
        "  predicted_label = output[0]\n",
        "  print(f\"Predicted Label: {label_count_dict[predicted_label]}\")\n",
        "pred(test_texts)\n",
        "pred(test_texts_1)\n",
        "pred(test_texts_2)\n",
        "pred(test_texts_3)\n",
        "\n",
        "\"\"\"\n",
        "Predicted Label: Hypertension\n",
        "Predicted Label: Varicose Veins\n",
        "Predicted Label: allergy\n",
        "Predicted Label: Chicken pox\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KotqExFEg72e"
      },
      "outputs": [],
      "source": [
        "label_count_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZMWivkh5QyE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkvFgr61elvvj5TrVG2jgD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}